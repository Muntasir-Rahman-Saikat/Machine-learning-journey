{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-11-30T10:21:11.371750Z",
     "start_time": "2025-11-30T10:21:11.367299Z"
    }
   },
   "source": [
    "train_path=r\"C:\\Users\\User\\Downloads\\60 days of python\\day-38(Aspect base sentiment analysis)\\train.csv\"\n",
    "test_path=r'C:\\Users\\User\\Downloads\\60 days of python\\day-38(Aspect base sentiment analysis)\\test.csv'"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-30T10:21:11.388034Z",
     "start_time": "2025-11-30T10:21:11.381792Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ],
   "id": "25a89a2bdf88647b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-30T10:21:12.216617Z",
     "start_time": "2025-11-30T10:21:11.416343Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "train_df=pd.read_csv(train_path)\n",
    "test_df=pd.read_csv(test_path)"
   ],
   "id": "6ab6bf7aa6ce47ba",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-30T10:21:12.290919Z",
     "start_time": "2025-11-30T10:21:12.272092Z"
    }
   },
   "cell_type": "code",
   "source": "train_df.head()",
   "id": "9583ee2ab7ce217",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                              review   aspect sentiment\n",
       "0              But the staff was so horrible to us .    staff  negative\n",
       "1  To be completely fair , the only redeeming fac...     food  positive\n",
       "2  The food is uniformly exceptional , with a ver...     food  positive\n",
       "3  The food is uniformly exceptional , with a ver...  kitchen  positive\n",
       "4  The food is uniformly exceptional , with a ver...     menu   neutral"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>aspect</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>But the staff was so horrible to us .</td>\n",
       "      <td>staff</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>To be completely fair , the only redeeming fac...</td>\n",
       "      <td>food</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The food is uniformly exceptional , with a ver...</td>\n",
       "      <td>food</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The food is uniformly exceptional , with a ver...</td>\n",
       "      <td>kitchen</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The food is uniformly exceptional , with a ver...</td>\n",
       "      <td>menu</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-30T10:21:12.321316Z",
     "start_time": "2025-11-30T10:21:12.310197Z"
    }
   },
   "cell_type": "code",
   "source": "train_df[train_df['sentiment']=='negative'].index",
   "id": "cbcf325185f8335c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([   0,   14,   15,   16,   18,   29,   30,   39,   49,   94,\n",
       "       ...\n",
       "       3543, 3549, 3550, 3563, 3564, 3569, 3580, 3582, 3585, 3590],\n",
       "      dtype='int64', length=807)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-30T10:21:12.383053Z",
     "start_time": "2025-11-30T10:21:12.373269Z"
    }
   },
   "cell_type": "code",
   "source": "train_df[train_df['sentiment']=='negative']['review'][14]",
   "id": "a22ae5de23d0a959",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'They did not have mayonnaise , forgot our toast , left out ingredients ( ie cheese in an omelet ) , below hot temperatures and the bacon was so over cooked it crumbled on the plate when you touched it .'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-30T10:21:12.515432Z",
     "start_time": "2025-11-30T10:21:12.502743Z"
    }
   },
   "cell_type": "code",
   "source": "train_df.value_counts('sentiment')",
   "id": "876e00d387c41faa",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentiment\n",
       "positive    2164\n",
       "negative     807\n",
       "neutral      637\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-30T10:21:12.591938Z",
     "start_time": "2025-11-30T10:21:12.585080Z"
    }
   },
   "cell_type": "code",
   "source": "test_df.value_counts(\"sentiment\")",
   "id": "9a338e8a46d7ee88",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentiment\n",
       "positive    727\n",
       "negative    196\n",
       "neutral     196\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-30T10:21:12.870154Z",
     "start_time": "2025-11-30T10:21:12.859259Z"
    }
   },
   "cell_type": "code",
   "source": "train_df.value_counts('aspect')",
   "id": "4ce151ec8be86654",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "aspect\n",
       "food                        340\n",
       "service                     189\n",
       "place                        59\n",
       "menu                         56\n",
       "prices                       56\n",
       "                           ... \n",
       "Halibut                       1\n",
       "Indian Fast Food              1\n",
       "waiting area                  1\n",
       "Indian dining experience      1\n",
       ", chicken                     1\n",
       "Name: count, Length: 1322, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Data",
   "id": "511cb7ab28ac8f11"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-30T10:21:13.063836Z",
     "start_time": "2025-11-30T10:21:13.057560Z"
    }
   },
   "cell_type": "code",
   "source": [
    "index = 0\n",
    "example = train_df.iloc[index]\n",
    "\n",
    "print('Review:', example['review'])\n",
    "print('Aspect:', example['aspect'])\n",
    "print('Sentiment:', example['sentiment'])"
   ],
   "id": "d11c9bb38eeaae58",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review: But the staff was so horrible to us .\n",
      "Aspect: staff\n",
      "Sentiment: negative\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-30T10:21:13.296104Z",
     "start_time": "2025-11-30T10:21:13.290254Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Normalization of the text\n",
    "import re\n",
    "\n",
    "text = example['review']\n",
    "# lowercase\n",
    "text = text.lower()\n",
    "# remove punctuations, special characters\n",
    "text = re.sub(r\"[^a-z0-9\\s]\", '', text)#means a-z and 0-9 egula chara onno kono value(like punchuation) thake segulo replace kore dibo empty(\"\") string diye.\"^\" sign ta bujay j a-z and 0-9 egula chara bakigula bad diye and \\s mane bujay j space ta thakuk\n",
    "# remove extra whitespace\n",
    "text = ' '.join(text.split())\n",
    "print('Review:', text)"
   ],
   "id": "60b280a7891c1bd8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review: but the staff was so horrible to us\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-30T10:21:13.366277Z",
     "start_time": "2025-11-30T10:21:13.360759Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Normalize the text\n",
    "def normalize(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z0-9\\s]', '', text)\n",
    "    text = ' '.join(text.split())\n",
    "    return text\n",
    "\n",
    "text = example['review']\n",
    "normalized_text = normalize(text)\n",
    "print(normalized_text)"
   ],
   "id": "ebd7db191b33a11c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "but the staff was so horrible to us\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-30T10:21:13.434191Z",
     "start_time": "2025-11-30T10:21:13.428499Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\" Tokenization\n",
    "Definition: Token is any sequential part of a text that collectively make up the entire text\n",
    "\n",
    "Text = \"but the staff was so horrible to us\"\n",
    "\n",
    "Word-level Tokens: \"but\", \"the\", \"staff\", \"was\", \"so\", \"horrible\", \"to\", \"us\"\n",
    "Character level tokens: b,u,t,' ',t,h,e,' ',s,t,a,f,f,' ',w,a,s,' ',s,o, ' ',h,o,r,r,i,b,l,e,' ',t,o,' ',u,s\n",
    "Subword: but, the, sta, ff, was, so, horr, ible, to us\n",
    "\n",
    "For example: text = but the staff was so horrible to us\n",
    "Tokens: \"but\", \"the\", \"staff\", \"was\", \"so\", \"horrible\", \"to\", \"use\" (word level tokenization)\n",
    "Standard tokenization technique: Character-level tokenization\n",
    "For example: text = \"Life is good\"\n",
    "Character level tokens: 'L', 'i', 'f', 'e', ' ', 'g', 'o', 'o', 'd'\n",
    "Advanced tokenization technique: BPE (Byte-Pair Encoding)\n",
    "For example: text = \"life is good\"\n",
    "Bypte pair tokens: li, fe, 'is', 'goo', 'd' #basically means subword tokanization\n",
    "\"\"\"\n",
    "\n",
    "def tokenize(text):\n",
    "    tokens = text.split()\n",
    "    return tokens\n",
    "\n",
    "tokens = tokenize(normalized_text)\n",
    "print(tokens)"
   ],
   "id": "825b2a1b11201589",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['but', 'the', 'staff', 'was', 'so', 'horrible', 'to', 'us']\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-30T10:21:13.553972Z",
     "start_time": "2025-11-30T10:21:13.525036Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#prottekta token ekta string but model receives only number.so each token is set to an number\n",
    "\"\"\"suppose your `train_df['review']` contains:\n",
    "\n",
    "texts = [\n",
    "    \"I love pizza\",\n",
    "    \"I love burgers\",\n",
    "    \"Pizza is tasty\"\n",
    "]\n",
    "\n",
    "Initial vocabulary:\n",
    "token_2_id = {\n",
    "    \"<PAD>\": 0,\n",
    "    \"<UNK>\": 1\n",
    "}\n",
    "idx = 2\n",
    "\n",
    "PROCESS:\n",
    "\n",
    "Text 1: \"I love pizza\"\n",
    "Tokens: [\"i\", \"love\", \"pizza\"]\n",
    "\n",
    "i ‚Üí add as 2\n",
    "love ‚Üí add as 3\n",
    "pizza ‚Üí add as 4\n",
    "\n",
    "Text 2: \"I love burgers\"\n",
    "Tokens: [\"i\", \"love\", \"burgers\"]\n",
    "\n",
    "i ‚Üí already exists\n",
    "love ‚Üí already exists\n",
    "burgers ‚Üí add as 5\n",
    "\n",
    "Text 3: \"Pizza is tasty\"\n",
    "Tokens: [\"pizza\", \"is\", \"tasty\"]\n",
    "\n",
    "pizza ‚Üí exists\n",
    "is ‚Üí add as 6\n",
    "tasty ‚Üí add as 7\n",
    "\n",
    "FINAL VOCABULARY:\n",
    "{\n",
    "  \"<PAD>\": 0,\n",
    "  \"<UNK>\": 1,\n",
    "  \"i\": 2,\n",
    "  \"love\": 3,\n",
    "  \"pizza\": 4,\n",
    "  \"burgers\": 5,\n",
    "  \"is\": 6,\n",
    "  \"tasty\": 7\n",
    "}\n",
    "\n",
    "Vocabulary size = 8\n",
    "\"\"\"\n",
    "def build_vocab(texts):\n",
    "    token_2_id = {\n",
    "        '<PAD>': 0,\n",
    "        '<UNK>': 1,\n",
    "    }\n",
    "\n",
    "    idx = 2\n",
    "    for text in texts:\n",
    "        normalized_text = normalize(text)\n",
    "        tokens = tokenize(normalized_text)\n",
    "\n",
    "        for token in tokens:\n",
    "            if token_2_id.get(token) is None:\n",
    "                token_2_id[token] = idx\n",
    "                idx += 1\n",
    "    return token_2_id\n",
    "\n",
    "token_2_id = build_vocab(train_df['review'])\n",
    "print(\"Vocabulary size:\", len(token_2_id))"
   ],
   "id": "deb7c612efd34d43",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 3800\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-30T10:21:13.604753Z",
     "start_time": "2025-11-30T10:21:13.599445Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def convert_tokens_2_ids(tokens):\n",
    "    input_ids = [\n",
    "        token_2_id.get(token, token_2_id['<UNK>']) for token in tokens\n",
    "    ]\n",
    "    return input_ids\n",
    "\n",
    "text = example['review'] + \" hello \"\n",
    "normalized_text = normalize(text)\n",
    "tokens = tokenize(normalized_text)\n",
    "input_ids = convert_tokens_2_ids(tokens)\n",
    "\n",
    "print(\"Vocabulary size:\", len(token_2_id))\n",
    "print(tokens)\n",
    "print(input_ids)"
   ],
   "id": "ab8d67b04b43a935",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 3800\n",
      "['but', 'the', 'staff', 'was', 'so', 'horrible', 'to', 'us', 'hello']\n",
      "[2, 3, 4, 5, 6, 7, 8, 9, 1]\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-30T10:21:13.674281Z",
     "start_time": "2025-11-30T10:21:13.668802Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pickle\n",
    "with open(\"vocab.pkl\", \"wb\") as f:#Opens a file named vocab.pkl in write-binary mode and assigns it to f.\n",
    "    pickle.dump(token_2_id, f)#Saves the object token_2_id into the file using pickle.\n",
    "label_map = {\n",
    "    \"negative\": 0,\n",
    "    \"neutral\": 1,\n",
    "    \"positive\": 2,\n",
    "}"
   ],
   "id": "dce06201e758e429",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-30T10:21:13.712266Z",
     "start_time": "2025-11-30T10:21:13.706168Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "\n",
    "\n",
    "\n",
    "class ABSADataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):#automatically called by dataloader\n",
    "        example = train_df.iloc[idx]\n",
    "        text = example['review']\n",
    "        aspect = example['aspect']\n",
    "        sentiment = example['sentiment']\n",
    "\n",
    "        text_aspect_pair = text + ' ' + aspect\n",
    "        normalized_text = normalize(text_aspect_pair)\n",
    "        tokens = tokenize(normalized_text)\n",
    "        input_ids = convert_tokens_2_ids(tokens)\n",
    "        label_id = label_map[sentiment]\n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"label\": label_id\n",
    "        }\n",
    "\n",
    "\n",
    "# Batch: Is a collection of examples\n",
    "# Typically 32, 64, 128 etc.\n",
    "# In a batch all the examples shape must match\n",
    "# pixels, label"
   ],
   "id": "78c0b95da915af6e",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-30T10:21:13.724039Z",
     "start_time": "2025-11-30T10:21:13.719431Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\"batch_input_ids = [ #batch size number ta dataloader bole dilei hobe..baki kaj ta dataloader nije handle kore nibe\n",
    "    [5, 7, 9],\n",
    "    [4, 2],\n",
    "    [8, 3, 6, 1]\n",
    "]\n",
    "\n",
    "batch_labels = [0, 1, 0]\n",
    "\n",
    "Step 2: Find max_len\n",
    "Lengths ‚Üí 3, 2, 4\n",
    "max_len = 4\n",
    "\n",
    "Step 3: Pad with pad_token_id = 0\n",
    "\n",
    "[5, 7, 9]     ‚Üí [5, 7, 9, 0]\n",
    "[4, 2]        ‚Üí [4, 2, 0, 0]\n",
    "[8, 3, 6, 1]  ‚Üí [8, 3, 6, 1]\n",
    "\n",
    "batch_padded_input_ids = [\n",
    "    [5, 7, 9, 0],\n",
    "    [4, 2, 0, 0],\n",
    "    [8, 3, 6, 1]\n",
    "]\n",
    "\n",
    "Step 4: Convert to tensors\n",
    "\n",
    "batch_input_ids tensor:\n",
    "tensor([\n",
    "    [5, 7, 9, 0],\n",
    "    [4, 2, 0, 0],\n",
    "    [8, 3, 6, 1]\n",
    "])\n",
    "\n",
    "batch_labels tensor:\n",
    "tensor([0, 1, 0])\n",
    "\n",
    "Final output:\n",
    "{\n",
    "    \"batch_input_ids\": tensor([[5, 7, 9, 0],\n",
    "                               [4, 2, 0, 0],\n",
    "                               [8, 3, 6, 1]]),\n",
    "    \"batch_labels\": tensor([0, 1, 0])\n",
    "}\"\"\"\n",
    "@staticmethod\n",
    "def collate_fn(batch): #when to use this collab function ? when each batch size is different\n",
    "    # batch size i.e. 32\n",
    "    # [{input_ids: [], label: 0}, ....{input_ids: [], label: 1}]\n",
    "    batch_input_ids = [item['input_ids'] for item in batch]\n",
    "    batch_labels = [item['label'] for item in batch]\n",
    "\n",
    "    max_len = max(len(input_ids) for input_ids in batch_input_ids)\n",
    "    pad_token_id = token_2_id['<PAD>']\n",
    "    batch_padded_input_ids = [\n",
    "            input_ids + [pad_token_id] * (max_len - len(input_ids)) for input_ids in batch_input_ids\n",
    "    ]\n",
    "\n",
    "    return {\n",
    "        \"batch_input_ids\": torch.tensor(\n",
    "            batch_padded_input_ids,\n",
    "            dtype=torch.long\n",
    "        ),\n",
    "        \"batch_labels\": torch.tensor(\n",
    "            batch_labels,\n",
    "            dtype=torch.long\n",
    "        )\n",
    "    }"
   ],
   "id": "b91fd7fcf3c6cc70",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-30T10:21:51.125079Z",
     "start_time": "2025-11-30T10:21:51.119773Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_ds=ABSADataset(train_df)\n",
    "train_ds.__getitem__(3)"
   ],
   "id": "a5f2a833576950ee",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [3,\n",
       "  16,\n",
       "  30,\n",
       "  31,\n",
       "  32,\n",
       "  33,\n",
       "  34,\n",
       "  35,\n",
       "  36,\n",
       "  37,\n",
       "  17,\n",
       "  38,\n",
       "  39,\n",
       "  40,\n",
       "  23,\n",
       "  41,\n",
       "  42,\n",
       "  43,\n",
       "  44,\n",
       "  45,\n",
       "  46,\n",
       "  47,\n",
       "  48,\n",
       "  49,\n",
       "  3,\n",
       "  50,\n",
       "  51,\n",
       "  52,\n",
       "  37],\n",
       " 'label': 2}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-26T13:34:49.493099Z",
     "start_time": "2025-11-26T13:34:49.485375Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pytorch_lightning as pl\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "class ABSADataModule(pl.LightningDataModule):#ekhane dataset class and dataloader same clss er under e niye ansi\n",
    "    def __init__(self, train_path, test_path, batch_size):\n",
    "        super().__init__()\n",
    "        self.train_path = train_path\n",
    "        self.test_path = test_path\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def setup(self, stage=None):#ei function ta ekdom surute auto call hoy.Suppose ,dataset ta cloud theke download korar lagte pare or dataset ta training er age onno kono kaj korar lagte par\n",
    "        # prepare the dataset for training\n",
    "        # You may download, process some stuff\n",
    "        # Everything data related we need to do before training\n",
    "        # Read the dataset\n",
    "        train_df = pd.read_csv(self.train_path)\n",
    "        test_df = pd.read_csv(self.test_path)\n",
    "\n",
    "        # build vocabulary\n",
    "        self.token_2_id = build_vocab(train_df['review'])\n",
    "        # Initialize the dataset\n",
    "        self.train_set = ABSADataset(train_df)\n",
    "        self.test_set = ABSADataset(test_df)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.train_set,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,#THAT means prottekta epoch er surute batch er value gulo shuffle hobe\n",
    "            collate_fn=self.collate_fn\n",
    "        )\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.test_set,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            collate_fn=self.collate_fn\n",
    "        )\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def collate_fn(batch):\n",
    "        # batch size i.e. 32\n",
    "        # [{input_ids: [], label: 0}, ....{input_ids: [], label: 1}]\n",
    "        batch_input_ids = [item['input_ids'] for item in batch]\n",
    "        batch_labels = [item['label'] for item in batch]\n",
    "\n",
    "        max_len = max(len(input_ids) for input_ids in batch_input_ids)\n",
    "        pad_token_id = token_2_id['<PAD>']\n",
    "        batch_padded_input_ids = [\n",
    "                input_ids + [pad_token_id] * (max_len - len(input_ids)) for input_ids in batch_input_ids\n",
    "        ]\n",
    "\n",
    "        return {\n",
    "            \"batch_input_ids\": torch.tensor(\n",
    "                batch_padded_input_ids,\n",
    "                dtype=torch.long\n",
    "            ),\n",
    "            \"batch_labels\": torch.tensor(\n",
    "                batch_labels,\n",
    "                dtype=torch.long\n",
    "            )\n",
    "        }\n"
   ],
   "id": "b9e58340dcd04024",
   "outputs": [],
   "execution_count": 79
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-26T13:34:49.504959Z",
     "start_time": "2025-11-26T13:34:49.501675Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data_module=ABSADataModule(\n",
    "    train_path=train_path,\n",
    "    test_path=test_path,\n",
    "    batch_size=32,\n",
    ")"
   ],
   "id": "982c3b5e0caaae95",
   "outputs": [],
   "execution_count": 80
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# **Model**",
   "id": "75f96a3031a7a33a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-26T13:34:49.521716Z",
     "start_time": "2025-11-26T13:34:49.511952Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch import optim\n",
    "from torchmetrics.classification import Accuracy\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "# class ABSA(nn.Module):\n",
    "#     def __init__(self, vocab_size, num_labels=3):\n",
    "#         super(ABSA, self).__init__()\n",
    "#         self.vocab_size = vocab_size\n",
    "#         self.num_labels = num_labels\n",
    "#         self.embedding_layer = nn.Embedding(\n",
    "#             num_embeddings=vocab_size, embedding_dim=256\n",
    "#         )\n",
    "#         # Sequence to sequence learning\n",
    "#         self.lstm_layer = nn.LSTM(\n",
    "#             input_size=256,\n",
    "#             hidden_size=512,\n",
    "#             batch_first=True, # as batch is first dimension in the input shape\n",
    "#         )\n",
    "#\n",
    "#         self.fc_layer = nn.Linear(\n",
    "#             in_features=512,\n",
    "#             out_features=self.num_labels\n",
    "#         )\n",
    "#     def forward(self, x):\n",
    "#         # input: (B, L)\n",
    "#         embeddings = self.embedding_layer(x) # (B, L, 256)\n",
    "#         lstm_out, _ = self.lstm_layer(embeddings) # (B, L, 512)\n",
    "#         logits = self.fc_layer(lstm_out[:, -1, :])\n",
    "#         return logits\n",
    "\n",
    "class ABSAModel(pl.LightningModule):\n",
    "    def __init__(self, vocab_size, num_labels=3):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.num_labels = num_labels\n",
    "        \"\"\"Natural Language Processing e tokenization er pore  sobcheye significant layer hocche embedding layer.Embedding layer hocche ekta technique jekhane amra prottekta token k ekta boro list of numbers diye represent kori \"\"\"\n",
    "        # Embedding: Converts each token id into a vector of numbers\n",
    "        # For example embedding size = 256\n",
    "        # token: apple => 15 => [14.5, 12.5, ....., 1.25] of shape 256\n",
    "        # [token_id][vector] => vocab_size x embedding dimension\n",
    "        # For example: vocab_size=10, embedding_dim=5\n",
    "        # [0] = [......]\n",
    "        # [1] = [......]\n",
    "        # .....\n",
    "        # Example: I eat apple and orange.\n",
    "        # Consider apple and orange\n",
    "        #  # input ids shape = (B, 3,)\n",
    "        # After that, shape = (B, L, 256)\n",
    "        \"\"\"\"why we use embedding\n",
    "        I love pizza\" ‚Üí [2, 5, 8]\n",
    "\n",
    "        carry **no meaning**.\n",
    "        ID 2 is not more similar to ID 5 ‚Äî they are just labels.\n",
    "\n",
    "        So we use **embeddings**.\n",
    "\n",
    "\n",
    "        # ‚≠ê WHAT IS AN EMBEDDING?\n",
    "\n",
    "        An embedding converts each word/tok into a **dense vector**:\n",
    "\n",
    "        pizza ‚Üí [0.21, -0.11, 0.87, ...]\n",
    "        burger ‚Üí [0.19, -0.09, 0.92, ...]\n",
    "        \"\"\"\n",
    "        self.embedding_layer = nn.Embedding(\n",
    "            num_embeddings=self.vocab_size,#means kotogula token er jonno embedding korte hobe\n",
    "            embedding_dim=256\n",
    "        )\n",
    "        \"\"\"tensor([[ 0.12, -0.34,  0.56,  0.78.....256 ta hobe],   # \"I\"\n",
    "        [ 0.09,  0.44, -0.21,  0.87.....256 ta hobe],              # \"love\"\n",
    "        [-0.11,  0.65,  0.32, -0.47.....256 ta hobe]])             # \"pizza\"\n",
    "\"\"\"\n",
    "        #sequence to sequence learning.means textual learning that means text theke kichu sikhte chacche.Modern architechture e transformer layer ta amra use kori\n",
    "        self.lstm_layer = nn.LSTM(\n",
    "            input_size=256,#cause ager layer theke 256 size asbe\n",
    "            hidden_size=512,#typically output size ta input size er 2 times hoy\n",
    "            batch_first=True, #jehetu amader processing e(B, L, 256) batch ta sobar age ase tai true diye disi\n",
    "\n",
    "        )\n",
    "        self.fc_layer = nn.Linear(\n",
    "            in_features=512,\n",
    "            out_features=self.num_labels#here num_labels=3 that means  # negative, neutral, positive scores\n",
    "        )\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "        self.save_hyperparameters()\n",
    "    def forward(self, x):\n",
    "        embeddings = self.embedding_layer(x)#X means number of batch.embedding output:(B,L,256)\n",
    "        lstm_out, _ = self.lstm_layer(embeddings)#output:(B,L,512)...\"_\" means hidden state er output jeta amder dorkar nai but layer nijer dorkar\n",
    "        logits = self.fc_layer(lstm_out[:,-1,:])    #lstm layer er output ta k fully connected layer e connect kore disi\n",
    "        #logits output: sobgulo batch nicche,lenght er sudu last value ta nicche and then output 512 sobgulai provide kortese\n",
    "        \"\"\"Now the question is why we are taking -1(infact why the last value).ok lets see\n",
    "        I eat rice\n",
    "        L=3\n",
    "\n",
    "        I=logits 1\n",
    "        I eat=logits2\n",
    "        I eat rice =logits3\n",
    "        another example:\n",
    "        sentence: I love this movie\n",
    "        | Token   | Output vector |\n",
    "        | ------- | ------------- |\n",
    "        | I ‚Üí     | h1            |\n",
    "        | love ‚Üí  | h2            |\n",
    "        | this ‚Üí  | h3            |\n",
    "        | movie ‚Üí | **h4**        |\n",
    "        We use h4, because it contains information from all previous steps.\n",
    "\n",
    "        \"\"\"\n",
    "        # self.model = ABSA(vocab_size, num_labels)\n",
    "        #\n",
    "\n",
    "        # self.save_hyperparameters()\n",
    "        return logits\n",
    "\n",
    "    def training_step(self, batch, batch_idx):#automatically called by lightning module in trainer.fit\n",
    "        input_ids = batch['batch_input_ids']\n",
    "        labels = batch['batch_labels']\n",
    "        input_ids = input_ids.to(device)\n",
    "        labels = labels.to(device)\n",
    "        logits = self.forward(input_ids) # Calls the forward method\n",
    "        loss = self.loss_fn(logits, labels)\n",
    "        self.log('train_loss', loss, prog_bar=True)\n",
    "        acc = self.compute_metrics(logits, labels)\n",
    "        self.log('train_acc', acc, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):#automatically called by lightning module in trainer.fit\n",
    "        input_ids = batch['batch_input_ids']\n",
    "        labels = batch['batch_labels']\n",
    "        input_ids = input_ids.to(device)\n",
    "        labels = labels.to(device)\n",
    "        logits = self.forward(input_ids) # Calls the forward method\n",
    "        loss = self.loss_fn(logits, labels)\n",
    "        self.log('test_loss', loss, prog_bar=True)\n",
    "        acc = self.compute_metrics(logits, labels)\n",
    "        self.log('test_acc', acc, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self) :#automatically called by lightning module in trainer.fit\n",
    "        return optim.Adam(self.parameters(), lr=3e-4)\n",
    "\n",
    "    def compute_metrics(self, logits, labels):\n",
    "        logits = logits.detach().cpu()\n",
    "        labels = labels.detach().cpu()\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        accuracy = Accuracy(task=\"multiclass\", num_classes=self.num_labels)\n",
    "        return accuracy(preds, labels)"
   ],
   "id": "8aa2cae5f71b9500",
   "outputs": [],
   "execution_count": 81
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-26T13:34:49.624655Z",
     "start_time": "2025-11-26T13:34:49.526723Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data_module.setup()\n",
    "model=ABSAModel(\n",
    "    vocab_size=len(data_module.token_2_id),\n",
    "    num_labels=3\n",
    ")"
   ],
   "id": "4fe4652c70d13e64",
   "outputs": [],
   "execution_count": 82
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# **Training**",
   "id": "6f7a6295d0ec246b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-26T13:36:53.946875Z",
     "start_time": "2025-11-26T13:34:49.638410Z"
    }
   },
   "cell_type": "code",
   "source": [
    "trainer=pl.Trainer(\n",
    "    max_epochs=10,\n",
    ")\n",
    "trainer.fit(model,data_module)\n"
   ],
   "id": "fe5acf6fa403fe68",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üí° Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "\n",
      "  | Name            | Type             | Params | Mode \n",
      "-------------------------------------------------------------\n",
      "0 | embedding_layer | Embedding        | 972 K  | train\n",
      "1 | lstm_layer      | LSTM             | 1.6 M  | train\n",
      "2 | fc_layer        | Linear           | 1.5 K  | train\n",
      "3 | loss_fn         | CrossEntropyLoss | 0      | train\n",
      "-------------------------------------------------------------\n",
      "2.6 M     Trainable params\n",
      "0         Non-trainable params\n",
      "2.6 M     Total params\n",
      "10.205    Total estimated model params size (MB)\n",
      "4         Modules in train mode\n",
      "0         Modules in eval mode\n",
      "C:\\Users\\User\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:433: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Training: |                                                                                              | 0/?‚Ä¶"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e3d11e97b5b54a48b053eae57f257407"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    }
   ],
   "execution_count": 83
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-26T15:09:19.384238Z",
     "start_time": "2025-11-26T15:09:16.222879Z"
    }
   },
   "cell_type": "code",
   "source": "trainer.test(model,data_module)",
   "id": "76d5b076c8387034",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:433: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Testing: |                                                                                               | 0/?‚Ä¶"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "bc76b060952a46c799f4432e9dafe4a6"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì\n",
       "‚îÉ\u001B[1m \u001B[0m\u001B[1m       Test metric       \u001B[0m\u001B[1m \u001B[0m‚îÉ\u001B[1m \u001B[0m\u001B[1m      DataLoader 0       \u001B[0m\u001B[1m \u001B[0m‚îÉ\n",
       "‚î°‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î©\n",
       "‚îÇ\u001B[36m \u001B[0m\u001B[36m        test_acc         \u001B[0m\u001B[36m \u001B[0m‚îÇ\u001B[35m \u001B[0m\u001B[35m   0.8811438679695129    \u001B[0m\u001B[35m \u001B[0m‚îÇ\n",
       "‚îÇ\u001B[36m \u001B[0m\u001B[36m        test_loss        \u001B[0m\u001B[36m \u001B[0m‚îÇ\u001B[35m \u001B[0m\u001B[35m   0.3562406301498413    \u001B[0m\u001B[35m \u001B[0m‚îÇ\n",
       "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì\n",
       "‚îÉ<span style=\"font-weight: bold\">        Test metric        </span>‚îÉ<span style=\"font-weight: bold\">       DataLoader 0        </span>‚îÉ\n",
       "‚î°‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î©\n",
       "‚îÇ<span style=\"color: #008080; text-decoration-color: #008080\">         test_acc          </span>‚îÇ<span style=\"color: #800080; text-decoration-color: #800080\">    0.8811438679695129     </span>‚îÇ\n",
       "‚îÇ<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>‚îÇ<span style=\"color: #800080; text-decoration-color: #800080\">    0.3562406301498413     </span>‚îÇ\n",
       "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "[{'test_loss': 0.3562406301498413, 'test_acc': 0.8811438679695129}]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 84
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-26T15:53:52.013650Z",
     "start_time": "2025-11-26T15:53:51.956220Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "# ----- 1. Set model to evaluation -----\n",
    "model.eval()\n",
    "\n",
    "# ----- 2. Input sentence -----\n",
    "sentence = train_df[train_df['sentiment']=='negative']['review'][ 3543]\n",
    "\n",
    "# Tokenize\n",
    "tokens = build_vocab(sentence)\n",
    "\n",
    "# Convert tokens ‚Üí IDs\n",
    "input_ids = [\n",
    "        token_2_id.get(token, token_2_id['<UNK>']) for token in tokens\n",
    "    ]\n",
    "\n",
    "# Pad\n",
    "\n",
    "# Convert to batch tensor\n",
    "input_tensor = torch.tensor([input_ids])\n",
    "\n",
    "# ----- 3. Predict -----\n",
    "with torch.no_grad():\n",
    "    logits = model(input_tensor)\n",
    "    predicted_label = torch.argmax(logits, dim=1)\n",
    "print(logits)\n",
    "\n",
    "print(\"Sentence:\", sentence)\n",
    "print(\"Predicted Label:\", predicted_label)\n"
   ],
   "id": "b24f1491f583cfe",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.8363, -1.0272,  1.6279]])\n",
      "Sentence: Kind of a small place but I guess if they are not too busy might be able to fit a group or kids .\n",
      "Predicted Label: tensor([2])\n"
     ]
    }
   ],
   "execution_count": 101
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "2246a2c8f49d32b"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
